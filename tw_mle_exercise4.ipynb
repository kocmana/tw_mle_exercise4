{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MLE - Exercise 2 - Comparative Experimentation\n",
    "## Andreas Kocman (se19m024)\n",
    "\n",
    "## Assignment\n",
    "This exercise follows very much the style of the previous exercise - you shall do experiments with different data sets and classifiers. Again, you can do the exercise alone, or in a group of two.\n",
    "\n",
    "The datasets to use are\n",
    "* The datasets from the exercise 3, i.e. Iris, Optical Digits (and if you are in a group, then also Breast Cancer)\n",
    "* Either the music or the image data set - decided by your matriculation number modulo 2, 0 means music, 1 means image (If you are doing this exercise in a group, then you shall take both data sets)\n",
    "\n",
    "The classifiers & parameters to use are\n",
    "* All the classifiers & parameters from exercise 3\n",
    "* Decision trees, you shall have two setups: one fully grown tree, and one setting for a pruned or pre-pruned tree.\n",
    "   * (If you are a group, you shall try a total of four settings: two unpruned trees using two different split criteria, and two setups for different amounts of (pre)-pruning the tree.)\n",
    "* Random Forests, using two different settings for the number of trees\n",
    "   * (If you are in a group, also vary the number of attributes that are used in each split; use three different values resp. computation methods (sqrt, log, fraction, ...); this should give you a total of 6 runs: (2 number of trees) x (3 number of attributes))\n",
    "* SVMs: just use the default settings, but use both SVC and LinearSVC classifiers (http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html, http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "\n",
    "### Image Dataset\n",
    "We will use the \"Fruit Image Dataset\", originally provided at http://data.vicos.si/datasets/FIDS30/, but with an edited version linked from Moodle (some images had an encoding not compatible with e.g. python libraries). Your task is to classify images into the category of fruit (a total of 30 defined categories) they belong to.\n",
    "\n",
    "As this is image data, feature extraction is a requirement before we can actually learn anything. As you shouldn't spend too much time on that, there is demo code on how to work with this data available, linked from the course main page.\n",
    "\n",
    "This code generates a set of 4 different features, all rather simple and based on histograms of colours (i.e. counts on how often a certain colour appears). You shall work with all four of them, and likely will see very different results.\n",
    "\n",
    "### Music Dataset\n",
    "We will use the dataset provided by George Tzanetakis, called \"gtzan\". This dataset contains 1.000 songs, 100 songs for 10 genres, and the task is therefore to predict the genres of a song; to limit file size, the songs are only 30 second snippets, and sampled with 22 khz only. You can download the dataset from the Moodle main page, or also at at http://kronos.ifs.tuwien.ac.at/GTZANmp3_22khz.zip. As this is copyrighted materials, please do not redistribute it...!\n",
    "\n",
    "As this is audio data, feature extraction is a requirement before we can actually learn anything. Therefore, there is demo code on how to work with this data available, linked from the course main page.\n",
    "\n",
    "This code generates different features, very simple ones containing just BeatsPerMinute, to more advanced ones based on advanced signal processing.  You shall work with all of them, and likely will see very different results.\n",
    "\n",
    "### Working in a group\n",
    "If you work in a group, as partially written above, your scope will be extended\n",
    "\n",
    "* More datasets: both music & image datasets\n",
    "* More parameter variations\n",
    "* More evaluation: for the Music&Image datasets, you shall also add an analysis of the confusion matrix for these datasets. It is sufficient, to provide one confusion matrix per feature set, you can select either the best classifier that you had on that feature set, or also other interesting ones.\n",
    "\n",
    "### Links for python\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "### Non-python feature extractors\n",
    "#### Java\n",
    "* For images, you can use a port of openCV, the OpenCV bindings (http://docs.opencv.org/2.4/doc/tutorials/introduction/desktop_java/java_dev_intro.html) or a different implementation in  Java: https://github.com/bytedeco/javacv. The sample code should then be quite similar to the one in python.\n",
    "* For music: http://jmir.sourceforge.net/index_jAudio.html (the jAudio component) offers a GUI for extracting features, best is to use BPM (strongest beat), MFCCs and Chroma, and their derivatives, i.e. the statistics that are also used in the sample code. jAudio should be able to generate ARFF files for WEKA.\n",
    "\n",
    "#### C#\n",
    "* For image, you should find OpenCV bindings as well for C#\n",
    "\n",
    "## Sources used\n",
    "* Scikit documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution\n",
    "\n",
    "### Datasets to Use\n",
    "Matriculation number: SE19M024%2 = 0 - using music dataset\n",
    "\n",
    "### Helper Functions for Solution and Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# global Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#sk learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "#Data reporting\n",
    "from IPython.display import display\n",
    "\n",
    "# Global definitions:\n",
    "averaging_approach = 'macro'\n",
    "zero_division_approach = 0\n",
    "number_of_folds = 5\n",
    "scoring = {'Accuracy': make_scorer(accuracy_score),\n",
    "            'Precision': make_scorer(precision_score, average=averaging_approach, zero_division=zero_division_approach),\n",
    "            'Recall': make_scorer(recall_score, average=averaging_approach, zero_division=zero_division_approach)}\n",
    "\n",
    "# Helper functions\n",
    "def parse_k_fold_results(results):\n",
    "    return \"m: \" + str(np.average(results)) + \" std: \" + str(np.std(results))\n",
    "\n",
    "def parse_argument_tuple_as_string(argumentsTuple):\n",
    "    return \"max Depth: \" + str(argumentsTuple[0])  + \\\n",
    "           \", min Samples: \" + str(argumentsTuple[1])\n",
    "\n",
    "def calculate_results_cross_validate(dataset_name, classifier_used, classifier_name, data, target):\n",
    "   scores = cross_validate(classifier_used, data, target,\n",
    "                                scoring = scoring,\n",
    "                                cv = number_of_folds,\n",
    "                                error_score = 0)\n",
    "\n",
    "   return pd.Series({\n",
    "            'dataset': dataset_name,\n",
    "            'classifier': classifier_name,\n",
    "            'arguments': str(classifier_used),\n",
    "            'mean_accuracy': np.average(scores.get('test_Accuracy')),\n",
    "            'mean_precision': np.average(scores.get('test_Precision')),\n",
    "            'mean_recall': np.average(scores.get('test_Recall')),\n",
    "            'accuracy': parse_k_fold_results(scores.get('test_Accuracy')),\n",
    "            'precision': parse_k_fold_results(scores.get('test_Precision')),\n",
    "            'recall':parse_k_fold_results(scores.get('test_Recall'))\n",
    "        })\n",
    "\n",
    "def print_results(array, column_for_max, ascending=False):\n",
    "    df = pd.DataFrame(array)\n",
    "    df = df.sort_values(by=[column_for_max], ascending=False)\n",
    "    display('Results', df)\n",
    "\n",
    "    best = df.iloc[df[column_for_max].argmax()]\n",
    "    display(best)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset Extraction Music"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# We need to construct our data set; unfortunately, we don't simply have a \"loadGTZanDataSet()\" function in SK-learn...\n",
    "# So we need to\n",
    "## Download our data set & extract it (one-time effort)\n",
    "## Run an audio feature extraction\n",
    "## Create the create the ground truth (label assignment, target, ...)\n",
    "\n",
    "\n",
    "# path to our audio folder\n",
    "# For the first run, download the images from http://kronos.ifs.tuwien.ac.at/GTZANmp3_22khz.zip, and unzip them to your folder\n",
    "imagePath=\"F:\\\\Informatik\\\\tw_mle_exercise4\\\\mp3\\\\\"\n",
    "\n",
    "\n",
    "# Find all songs in that folder; there are like 1.000 different ways to do this in Python, we chose this one :-)\n",
    "import glob, os\n",
    "print(os.getcwd())\n",
    "os.chdir(imagePath)\n",
    "fileNames = glob.glob(\"*/*.mp3\")\n",
    "numberOfFiles=len(fileNames)\n",
    "targetLabels=[]\n",
    "\n",
    "print (\"Found \" + str(numberOfFiles) + \" files\\n\")\n",
    "\n",
    "# The first step - create the ground truth (label assignment, target, ...)\n",
    "# For that, iterate over the files, and obtain the class label for each file\n",
    "# Basically, the class name is in the full path name, so we simply use that\n",
    "for fileName in fileNames:\n",
    "    pathSepIndex = fileName.index(\"\\\\\")\n",
    "    targetLabels.append(fileName[:pathSepIndex])\n",
    "\n",
    "# sk-learn can only handle labels in numeric format - we have them as strings though...\n",
    "# Thus we use the LabelEncoder, which does a mapping to Integer numbers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(targetLabels) # this basically finds all unique class names, and assigns them to the numbers\n",
    "print (\"Found the following classes: \" + str(list(le.classes_)))\n",
    "\n",
    "# now we transform our labels to integers\n",
    "target = le.transform(targetLabels);\n",
    "print (\"Transformed labels (first elements: \" + str(target[0:150]))\n",
    "\n",
    "# If we want to find again the label for an integer value, we can do something like this:\n",
    "# print list(le.inverse_transform([0, 18, 1]))\n",
    "\n",
    "print (\"... done label encoding\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Informatik\\tw_mle_exercise4\\mp3\n",
      "Found 1000 files\n",
      "\n",
      "Found the following classes: ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "Transformed labels (first elements: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n",
      "... done label encoding\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Before we extract the features, let's plot some information on a demo song, to illustrate what we are doing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from librosa import display\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "demoSongName = fileNames[1]\n",
    "demoSongPath = imagePath + demoSongName\n",
    "demoSongPath = demoSongPath.replace(\"\\\\\", \"/\")\n",
    "print(demoSongPath)\n",
    "print (\"Showing demo feature extraction on song \" + demoSongPath)\n",
    "\n",
    "y, sr = librosa.load(demoSongPath)\n",
    "\n",
    "# compute the tempo\n",
    "onset_env = librosa.onset.onset_strength(y, sr=sr)\n",
    "tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n",
    "print (\"The song has \" + str(tempo) + \" beats per minute\")\n",
    "\n",
    "# plot the wave form\n",
    "plt.figure()\n",
    "plt.subplot(3, 1, 1)\n",
    "librosa.display.waveplot(y, sr=sr)\n",
    "plt.title(demoSongPath)\n",
    "\n",
    "y_harm, y_perc = librosa.effects.hpss(y)\n",
    "plt.figure()\n",
    "plt.subplot(3, 1, 3)\n",
    "librosa.display.waveplot(y_harm, sr=sr, alpha=0.25)\n",
    "librosa.display.waveplot(y_perc, sr=sr, color='r', alpha=0.5)\n",
    "plt.title(demoSongPath + ': Harmonic + Percussive')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Plot the power spectrum\n",
    "plt.figure(figsize=(12, 8))\n",
    "D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n",
    "plt.subplot(4, 2, 1)\n",
    "librosa.display.specshow(D, y_axis='linear')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Linear-frequency power spectrogram')\n",
    "\n",
    "# Plot Chroma\n",
    "plt.figure()\n",
    "C = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "plt.subplot(4, 2, 5)\n",
    "librosa.display.specshow(C, y_axis='chroma')\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram')\n",
    "\n",
    "# Plot tempogram\n",
    "plt.figure()\n",
    "plt.subplot(4, 2, 8)\n",
    "Tgram = librosa.feature.tempogram(y=y, sr=sr)\n",
    "librosa.display.specshow(Tgram, x_axis='time', y_axis='tempo')\n",
    "plt.colorbar()\n",
    "plt.title('Tempogram')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:/Informatik/tw_mle_exercise4/mp3/blues/blues.00001.mp3\n",
      "Showing demo feature extraction on song F:/Informatik/tw_mle_exercise4/mp3/blues/blues.00001.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\informatik\\tw_mle_exercise4\\venv\\lib\\site-packages\\librosa\\core\\audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "ename": "NoBackendError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32mf:\\informatik\\tw_mle_exercise4\\venv\\lib\\site-packages\\librosa\\core\\audio.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[0;32m    128\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 129\u001B[1;33m         \u001B[1;32mwith\u001B[0m \u001B[0msf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSoundFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msf_desc\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    130\u001B[0m             \u001B[0msr_native\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msf_desc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msamplerate\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\informatik\\tw_mle_exercise4\\venv\\lib\\site-packages\\soundfile.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001B[0m\n\u001B[0;32m    628\u001B[0m                                          format, subtype, endian)\n\u001B[1;32m--> 629\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_file\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode_int\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclosefd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    630\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0missuperset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'r+'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseekable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\informatik\\tw_mle_exercise4\\venv\\lib\\site-packages\\soundfile.py\u001B[0m in \u001B[0;36m_open\u001B[1;34m(self, file, mode_int, closefd)\u001B[0m\n\u001B[0;32m   1183\u001B[0m         _error_check(_snd.sf_error(file_ptr),\n\u001B[1;32m-> 1184\u001B[1;33m                      \"Error opening {0!r}: \".format(self.name))\n\u001B[0m\u001B[0;32m   1185\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmode_int\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_snd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSFM_WRITE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\informatik\\tw_mle_exercise4\\venv\\lib\\site-packages\\soundfile.py\u001B[0m in \u001B[0;36m_error_check\u001B[1;34m(err, prefix)\u001B[0m\n\u001B[0;32m   1356\u001B[0m         \u001B[0merr_str\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_snd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msf_error_number\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1357\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprefix\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0m_ffi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstring\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merr_str\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'replace'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1358\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error opening 'F:/Informatik/tw_mle_exercise4/mp3/blues/blues.00001.mp3': File contains data in an unknown format.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNoBackendError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-473d38c022db>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[0mprint\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m\"Showing demo feature extraction on song \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mdemoSongPath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlibrosa\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdemoSongPath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;31m# compute the tempo\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\informatik\\tw_mle_exercise4\\venv\\lib\\site-packages\\librosa\\core\\audio.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001B[0m\n\u001B[0;32m    160\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstring_types\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    161\u001B[0m             \u001B[0mwarnings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwarn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'PySoundFile failed. Trying audioread instead.'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 162\u001B[1;33m             \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msr_native\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m__audioread_load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moffset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mduration\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    163\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    164\u001B[0m             \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreraise\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexc_info\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\informatik\\tw_mle_exercise4\\venv\\lib\\site-packages\\librosa\\core\\audio.py\u001B[0m in \u001B[0;36m__audioread_load\u001B[1;34m(path, offset, duration, dtype)\u001B[0m\n\u001B[0;32m    184\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 186\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0maudioread\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maudio_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0minput_file\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    187\u001B[0m         \u001B[0msr_native\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput_file\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msamplerate\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    188\u001B[0m         \u001B[0mn_channels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput_file\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchannels\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\informatik\\tw_mle_exercise4\\venv\\lib\\site-packages\\audioread\\__init__.py\u001B[0m in \u001B[0;36maudio_open\u001B[1;34m(path, backends)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m     \u001B[1;31m# All backends failed!\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 116\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mNoBackendError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNoBackendError\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Now we do the actual feature extraction\n",
    "import datetime\n",
    "from collections import deque\n",
    "import progressbar\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "\n",
    "# This is a helper function that computes the differences between adjacent array values\n",
    "def differences(seq):\n",
    "    iterable = iter(seq)\n",
    "    prev = next(iterable)\n",
    "    for element in iterable:\n",
    "        yield element - prev\n",
    "        prev = element\n",
    "\n",
    "# This is a helper function that computes various statistical moments over a series of values, including mean, median, var, min, max, skewness and kurtosis (a total of 7 values)\n",
    "def statistics(numericList):\n",
    "    return [np.mean(numericList), np.median(numericList), np.var(numericList), np.float64(st.skew(numericList)), np.float64(st.kurtosis(numericList)), np.min(numericList), np.max(numericList)]\n",
    "\n",
    "\n",
    "\n",
    "print (\"Extracting features using librosa\" + \" (\" + str(datetime.datetime.now()) + \")\")\n",
    "\n",
    "# compute some features based on BPMs, MFCCs, Chroma\n",
    "data_bpm=[]\n",
    "data_bpm_statistics=[]\n",
    "data_mfcc=[]\n",
    "data_chroma=[]\n",
    "\n",
    "# This takes a bit, so let's show it with a progress bar\n",
    "with progressbar.ProgressBar(max_value=len(fileNames)) as bar:\n",
    "    for indexSample, fileName in enumerate(fileNames):\n",
    "        # Load the audio as a waveform `y`, store the sampling rate as `sr`\n",
    "        y, sr = librosa.load(fileName)\n",
    "\n",
    "        # run the default beat tracker\n",
    "        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        # from this, we simply use the tempo as BPM feature\n",
    "        data_bpm.append([tempo])\n",
    "\n",
    "        # Then we compute a few statistics on the beat timings\n",
    "        beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "        # from the timings, compute the time differences between the beats\n",
    "        beat_intervals = np.array(deque(differences(beat_times)))\n",
    "\n",
    "        # And from this, take some statistics\n",
    "        # There might be a few files where the beat timings are not determined properly; we ignore them, resp. give them 0 values\n",
    "        if len(beat_intervals) < 1:\n",
    "            print (\"Errors with beat interval in file \" + fileName + \", index \" + str(indexSample) + \", using 0 values instead\")\n",
    "            data_bpm_statistics.append([tempo, 0, 0, 0, 0, 0, 0, 0])\n",
    "        else:\n",
    "            bpm_statisticsVector=[]\n",
    "            bpm_statisticsVector.append(tempo) # we also include the raw value of tempo\n",
    "            for stat in statistics(beat_intervals):  # in case the timings are ok, we actually compute the statistics\n",
    "                bpm_statisticsVector.append(stat) # and append it to the vector, which finally has 1 + 7 features\n",
    "            data_bpm_statistics.append(bpm_statisticsVector)\n",
    "\n",
    "        # Next feature are MFCCs; we take 12 coefficients; for each coefficient, we have around 40 values per second\n",
    "        mfccs=librosa.feature.mfcc(y=y, sr=sr, n_mfcc=12)\n",
    "        mfccVector=[]\n",
    "        for mfccCoefficient in mfccs: # we transform this time series by taking again statistics over the values\n",
    "            mfccVector.append(statistics(mfccCoefficient))\n",
    "\n",
    "        # Finally, this vector should have 12 * 7 features\n",
    "        data_mfcc.append(np.array(mfccVector).flatten())\n",
    "\n",
    "\n",
    "        # Last feature set - chroma (which is roughly similar to actual notes)\n",
    "        chroma=librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chromaVector=[]\n",
    "        for chr in chroma: # similar to before, we get a number of time-series\n",
    "            chromaVector.append(statistics(chr)) # and we resolve that by taking statistics over the time series\n",
    "        # Finally, this vector should be be 12 * 7 features\n",
    "        data_chroma.append(np.array(chromaVector).flatten())\n",
    "\n",
    "        bar.update(indexSample)\n",
    "\n",
    "print (\".... done\" + \" (\" + str(datetime.datetime.now()) + \")\")\n",
    "\n",
    "# Finally, we do classification\n",
    "# These are our feature sets; we will use each of them individually to train classifiers\n",
    "trainingSets = [data_bpm, data_bpm_statistics, data_chroma, data_mfcc ]\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import neighbors\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "\n",
    "# set up a number of classifiers\n",
    "classifiers = [neighbors.KNeighborsClassifier(),\n",
    "               naive_bayes.GaussianNB(),\n",
    "               tree.DecisionTreeClassifier(),\n",
    "               ensemble.RandomForestClassifier(),\n",
    "               svm.SVC(),\n",
    "               svm.LinearSVC(),\n",
    "              ]\n",
    "\n",
    "for indexDataset, train in enumerate(trainingSets):\n",
    "    for indexClassifier, classifier in enumerate(classifiers):\n",
    "        # do the actual classification\n",
    "        print (\"Classifying ...\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculation Functions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### k-NN Calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "def calculate_knn(dataset_name, data, target):\n",
    "    knn_results = []\n",
    "\n",
    "    n_neighbors = range(1,10,1)\n",
    "\n",
    "    for n in n_neighbors:\n",
    "        knn_classifier = neighbors.KNeighborsClassifier(n)\n",
    "        description = \"N = \" + str(n)\n",
    "        result = calculate_results_cross_validate(dataset_name,\n",
    "                                                  knn_classifier,\n",
    "                                                  \"knn\",\n",
    "                                                  data,\n",
    "                                                  target)\n",
    "        knn_results.append(result)\n",
    "    return knn_results\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bayes Calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "def calculate_bayes(dataset_name, data, target):\n",
    "    bayes_results = []\n",
    "\n",
    "    classifier = naive_bayes.CategoricalNB()\n",
    "    result = calculate_results_cross_validate(dataset_name,\n",
    "                                              classifier,\n",
    "                                              \"bayes\",\n",
    "                                              data,\n",
    "                                              target)\n",
    "    bayes_results.append(result)\n",
    "\n",
    "    return bayes_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Perceptron Calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def calculate_perceptron(dataset_name, data, target):\n",
    "    perceptron_results=[]\n",
    "    classifier = linear_model.Perceptron()\n",
    "    result = calculate_results_cross_validate(dataset_name,\n",
    "                                              classifier,\n",
    "                                              \"perceptron\",\n",
    "                                              data,\n",
    "                                              target)\n",
    "    perceptron_results.append(result)\n",
    "    return perceptron_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decision Tree Calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def calculate_decision_tree(dataset_name, data, target):\n",
    "    # Parameters for the decision tree\n",
    "    classifiers = [\n",
    "        DecisionTreeClassifier(),\n",
    "        DecisionTreeClassifier(max_depth = 5),\n",
    "        DecisionTreeClassifier(min_samples_leaf = 50)\n",
    "        ]\n",
    "    decision_tree_results = []\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        result = calculate_results_cross_validate(dataset_name,\n",
    "                                                  classifier,\n",
    "                                                  \"decision tree\",\n",
    "                                                  data,\n",
    "                                                  target)\n",
    "        decision_tree_results.append(result)\n",
    "    return decision_tree_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Forest Calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calculate_random_forest(dataset_name, data, target):\n",
    "\n",
    "    # Parameters for the random forest\n",
    "    arguments = range(10,200,50)\n",
    "    random_forest_results = []\n",
    "\n",
    "    for argument in arguments:\n",
    "        classifier = RandomForestClassifier(),\n",
    "        result = calculate_results_cross_validate(dataset_name,\n",
    "                                                  classifier,\n",
    "                                                  \"random forest\",\n",
    "                                                  data,\n",
    "                                                  target)\n",
    "        random_forest_results.append(result)\n",
    "    return random_forest_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM Calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import itertools\n",
    "\n",
    "def calculate_svm(dataset_name, data, target):\n",
    "    svm_results = []\n",
    "\n",
    "    classifiers = [\n",
    "        svm.SVC(),\n",
    "        svm.LinearSVC()\n",
    "    ]\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        result = calculate_results_cross_validate(dataset_name,\n",
    "                                                  classifier,\n",
    "                                                  \"svm\",\n",
    "                                                  data,\n",
    "                                                  target)\n",
    "        svm_results.append(result)\n",
    "    return svm_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Load Datasets\n",
    "from sklearn import datasets as sk_datasets\n",
    "\n",
    "# Iris and Digits\n",
    "datasets = [{'name': 'iris', 'data': sk_datasets.load_iris()},\n",
    "            {'name': 'digits', 'data': sk_datasets.load_digits()}]\n",
    "\n",
    "for dataset in datasets:\n",
    "    overall_results_dataset = []\n",
    "    name = dataset['name']\n",
    "    data = dataset['data']\n",
    "    overall_results_dataset.extend(calculate_knn(name, data.data, data.target))\n",
    "    #overall_results_dataset.extend(calculate_bayes(name, data.data, data.target))\n",
    "    overall_results_dataset.extend(calculate_perceptron(name, data.data, data.target))\n",
    "    overall_results_dataset.extend(calculate_decision_tree(name, data.data, data.target))\n",
    "    #overall_results_dataset.extend(calculate_random_forest(name, data.data, data.target))\n",
    "    overall_results_dataset.extend(calculate_svm(name, data.data, data.target))\n",
    "    overall_results_dataset = pd.DataFrame(overall_results_dataset)\n",
    "    dataset['result'] = overall_results_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Iris Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(datasets[0]['name'], datasets[0]['result'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Digits Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(datasets[1]['name'], datasets[1]['result'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}